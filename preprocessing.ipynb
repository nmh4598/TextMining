{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "nltk.download('stopwords')\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.n_tweet = 0  # Nombre de tweets  \n",
    "        self.n_tweet_doublons = 0 # Nombre de tweets doublons \n",
    "        self.n_de_mots = 0 # Nombre de mots dans tous les tweets\n",
    "        self.n_urls = 0  # Nombre de liens supprimés      \n",
    "        self.n_mentions = 0 # Nombre de mentions supprimés \n",
    "        self.n_hashtags = 0 # Nombre de hashtags supprimés \n",
    "        self.n_stopwords = 0 # Nombre de stopwords supprimés\n",
    "        self.entreprises = [\"LVMH\", \"L'Oréal\", \"Hermès\", \"TotalEnergies\", \n",
    "                            \"Sanofi\", \"Airbus\", \"Schneider Electric\", \"Capgemini\", \n",
    "                            \"Air liquide\", \"BNP Paribas\"]\n",
    " \n",
    "    def conservation_top_10(self):\n",
    "        \"\"\"\n",
    "            Conservation des 10 plus grosses entreprises du CAC40\n",
    "        \"\"\"\n",
    "        self.data = self.data[self.data.entreprise.isin(self.entreprises)]\n",
    "        self.n_tweet = len(self.data)\n",
    "\n",
    "    def compter_mots(self):\n",
    "        def compter_mots_chaque_ligne(text):\n",
    "            words = re.split(r\"[ ’'\\-]\", text)\n",
    "            return len(words)\n",
    "        \n",
    "        self.n_de_mots = self.data['text'].apply(compter_mots_chaque_ligne).sum()\n",
    "\n",
    "    def supprime_urls_hashtags_mentions_ponctuations_nombres(self):\n",
    "        \"\"\"\n",
    "        Supprime les liens https jusqu'à l'espace suivant\n",
    "        Supprime toutes les mentions (@) ainsi que la suite jusqu'à l'espace suivant\n",
    "        Supprime tous les Hashtags (#) ainsi que la suite jusqu'à l'espace suivant \n",
    "        Supprime tous les nombres\n",
    "        Supprime tous les pontuations\n",
    "        Supprime tous les accents\n",
    "        \"\"\"\n",
    "        url_regex = r'(http[s]?:\\/\\/\\S+)'\n",
    "        pattern = r'(@\\w+|#\\w+|http[s]?:\\/\\/\\S+)'\n",
    "        num_regex = r'\\d+'\n",
    "        \n",
    "        def clean(text):\n",
    "            items = re.findall(pattern, text)\n",
    "            mentions = [item for item in items if item.startswith('@')]\n",
    "            hashtags = [item for item in items if item.startswith('#')]\n",
    "            urls = [item for item in items if re.match(url_regex, item)]         \n",
    "            n_mentions = len(mentions)\n",
    "            n_hashtags = len(hashtags)\n",
    "            n_urls = len(urls)    \n",
    "            text = re.sub(pattern, '', text)\n",
    "            text = re.sub(num_regex, '', text)\n",
    "            punctuations = re.findall(r'[^\\w\\s]', text)  \n",
    "            n_pontuations = len(punctuations)\n",
    "            # Ice, faut remplace par l'espace \n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "                \n",
    "            return text, mentions, hashtags, urls,punctuations, n_mentions, n_hashtags, n_urls, n_pontuations\n",
    "\n",
    "        self.data['text'], self.data['mentions'], self.data['hashtags'], self.data['urls'], self.data['punctuations'], self.n_mentions, self.n_hashtags, self.n_urls,self.n_pontuations = zip(*self.data['text'].apply(clean))\n",
    "        self.data['text'] = self.data['text'].str.lower()\n",
    "        self.n_mentions = sum(self.n_mentions)\n",
    "        self.n_hashtags = sum(self.n_hashtags)\n",
    "        self.n_urls = sum(self.n_urls)\n",
    "        self.n_punctuations = sum(self.n_pontuations)\n",
    "        def remove_accents(text):\n",
    "            return unidecode(text)\n",
    "        self.data['text'] = self.data['text'].apply(remove_accents)\n",
    "\n",
    "    def supprime_les_stopwords(self):\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('french'))\n",
    "        liste_mot_non_voulu = [\"lvmh\", \"oreal\",\"oréal\",\"L’Oréal\", \"sanofi\", \"airbus\", \"totalenergie\", \"totalenergies\", \"total\", \"energie\", \"energies\", \"air\", \"liquide\", \"bnp\", \"paribas\", \"pariba\", \"airliquide\", \"ça\", \"ca\", \"faire\", \"moi\", \"deja\"]\n",
    "        stop_words_txt = pd.read_table(\"data/stop_words_french.txt\", header = 0)\n",
    "        list_sw = stop_words_txt[\"vides\"].to_list()\n",
    "        stop_words.update(liste_mot_non_voulu)\n",
    "        stop_words.update(list_sw)\n",
    "        \n",
    "        def supprime_stopwords(text):\n",
    "            words = re.split(r\"[ ’'\\-]\", text)\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            filtered_stop_words = [word for word in words if word.lower() in stop_words]\n",
    "            return ' '.join(filtered_words),filtered_stop_words, len(words) - len(filtered_words)\n",
    "\n",
    "        self.data['text'],self.data[\"stopword\"], n_stopwords = zip(*self.data['text'].apply(supprime_stopwords))\n",
    "        self.n_stopwords = sum(n_stopwords)\n",
    "    \n",
    "    def transform_normal(self):\n",
    "        \"\"\"       \n",
    "            Supprime tous les accents     \n",
    "            Supprime les tweets n'ayant plus de texte. \n",
    "        \"\"\"\n",
    "        self.data['text'] = self.data['text'].str.strip()\n",
    "        self.data = self.data[self.data['text'] != '']\n",
    "        self.data['date'] = pd.to_datetime(self.data['date']).dt.date\n",
    "        \n",
    "    def supprime_doublons(self):\n",
    "        \"\"\"\n",
    "            Supprime tous les doublons avec le même texte\n",
    "        \"\"\"\n",
    "        self.data = self.data.drop_duplicates([\"text\"])\n",
    "        self.n_tweet_sans_doublons = len(self.data)\n",
    "        self.n_tweet_doublons = self.n_tweet - self.n_tweet_sans_doublons\n",
    "    \n",
    "    def stats_desc(self):\n",
    "        \"\"\"\n",
    "            Retourne un dataframe contenant les résultats\n",
    "        \"\"\"\n",
    "        results_dict = {\n",
    "            \"Nombre de tweets\": [self.n_tweet],           \n",
    "            \"Nombre de liens\": [self.n_urls],\n",
    "            \"Nombre de mots\":[self.n_de_mots],\n",
    "            \"Nombre de stop word\":[self.n_stopwords],\n",
    "            \"Nombre de mentions\": [self.n_mentions],\n",
    "            \"Nombre de hashtags\": [self.n_hashtags],\n",
    "            \"Nombre de tweets doublons\": [self.n_tweet_doublons]\n",
    "\n",
    "            }\n",
    "        self.results_dict = pd.DataFrame(results_dict)\n",
    "        return self.results_dict\n",
    "    \n",
    "    def lemmatize(self, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "        nlp = spacy.load(\"fr_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "        self.data['text'] = self.data['text'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if token.pos_ in allowed_postags]))\n",
    "        \n",
    "    def sentiment_analysis(self):\n",
    "        def getSubjectivity(text):\n",
    "            \"\"\" \n",
    "                La propriété sentiment d'un objet TextBlob renvoie un tuple nommé de la \n",
    "                forme (subjectivité), où la subjectivité est un flottant compris entre 0,0 \n",
    "                et 1,0, indiquant la subjectivité du texte. Une subjectivité de 0,0 signifie \n",
    "                que le texte est très objectif et factuel, tandis qu'une subjectivité de \n",
    "                1,0 signifie que le texte est très subjectif et opiniâtre.\n",
    "            \"\"\"\n",
    "            return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "\n",
    "        def getPolarity(text):\n",
    "            \"\"\"\n",
    "                La propriété sentiment d'un objet TextBlob renvoie un tuple nommé de la forme \n",
    "                (polarity ), où polarity est un flottant compris entre -1.0 et 1.0, indiquant\n",
    "                la polarité de sentiment du texte. Une polarité de -1,0 est très négative,\n",
    "                0 est neutre et 1,0 est très positive.\n",
    "            \"\"\"\n",
    "            return TextBlob(text).sentiment.polarity\n",
    "\n",
    "        self.data[\"Subjectivité\"] = self.data[\"text\"].apply(getSubjectivity)\n",
    "        self.data[\"Polarité\"] = self.data[\"text\"].apply(getPolarity)\n",
    "\n",
    "\n",
    "        def getAnalysis_polarity(score):\n",
    "            if score < 0:\n",
    "                return \"Negative\"\n",
    "            elif score == 0:\n",
    "                return \"Neutral\"\n",
    "            else:\n",
    "                return \"Positive\"\n",
    "        def getAnalysis_subjectivity(score):\n",
    "            if score < 0.5:\n",
    "                return \"Très objectif et factuel\"\n",
    "            else:\n",
    "                return \"Très subjectif et opiniâtre\"\n",
    "        self.data[\"Subjectivité\"] = self.data[\"Subjectivité\"].apply(getAnalysis_subjectivity)\n",
    "        self.data[\"Polarité\"] = self.data[\"Polarité\"].apply(getAnalysis_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):  \n",
    "    data_processor = DataProcessor(data)\n",
    "    data_init = data_processor.data.copy()\n",
    "    data_processor.conservation_top_10()\n",
    "    data_processor.compter_mots()\n",
    "    data_processor.supprime_urls_hashtags_mentions_ponctuations_nombres()\n",
    "    #data_processor.lemmatize() # Dans le cas que on veut lemmatizer les données \n",
    "    data_processor.supprime_les_stopwords()\n",
    "    data_processor.transform_normal()\n",
    "    data_processor.supprime_doublons()\n",
    "    data_processor.sentiment_analysis()\n",
    "    data_fin = data_processor.data\n",
    "    data_processor.stats_desc()\n",
    "    results = data_processor.results_dict\n",
    "    return data_init, data_fin, results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43518\n",
      "19177\n",
      "   Nombre de tweets  Nombre de liens  Nombre de mots  Nombre de stop word  \\\n",
      "0             23531            14670          604278               274714   \n",
      "\n",
      "   Nombre de mentions  Nombre de hashtags  Nombre de tweets doublons  \n",
      "0               39081               14883                       4354  \n"
     ]
    }
   ],
   "source": [
    "path = Path.cwd().joinpath('data')\n",
    "path_data = path.joinpath('df.tweet.gzip')\n",
    "data = pd.read_parquet(path_data) \n",
    "data_init, data_fin, results = transform(data)\n",
    "print(len(data_init))\n",
    "print(len(data_fin))\n",
    "print(results)\n",
    "# data_init.to_parquet('data/data_init.parquet')\n",
    "# data_fin.to_parquet('data/data_fin.parquet')\n",
    "# results.to_parquet('data/results.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from collections import Counter\n",
    "\n",
    "class Stats_desc:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.n_tweet = 0  # Nombre de tweets  \n",
    "        self.n_de_mots = 0 # Nombre de mots dans tous les tweets\n",
    "        self.n_urls = 0  # Nombre de liens supprimés      \n",
    "        self.n_mentions = 0 # Nombre de mentions supprimés \n",
    "        self.n_hashtags = 0 # Nombre de hashtags supprimés \n",
    "        self.n_stopwords = 0 # Nombre de stopwords supprimés\n",
    "        self.data_stats = None # Les dataframes statistiques dé données final\n",
    "        self.data_text = None # Les lists des lists tous les mots \n",
    "        \n",
    "    def stats(self):  \n",
    "        def counts(data):     \n",
    "            word_counts = [len(s.split()) for s in self.data['text'].tolist()]\n",
    "            count = pd.DataFrame({'Entreprise': self.data[\"entreprise\"],\n",
    "                                  'Nombre de tweets': 1,\n",
    "                                  'Nombre de mots': word_counts,\n",
    "                                  'Mentions': self.data[\"mentions\"].apply(len),\n",
    "                                  'Hashtags': self.data[\"hashtags\"].apply(len),\n",
    "                                  'Urls': self.data[\"urls\"].apply(len),\n",
    "                                  'Ponctuations': self.data[\"punctuations\"].apply(len),\n",
    "                                  'Stopwords': self.data[\"stopword\"].apply(len)})\n",
    "            return count\n",
    "        self.data_stats = counts(self.data)\n",
    "        return self.data_stats \n",
    "    \n",
    "    def frequence(self, top_n):\n",
    "        def tokenize(docs):\n",
    "            tokenized_docs = []\n",
    "            for doc in docs:\n",
    "                tokens = gensim.utils.simple_preprocess(doc, deacc=True)\n",
    "                tokens = [token for token in tokens if len(token) > 1]\n",
    "                tokenized_docs.append(tokens)\n",
    "            return tokenized_docs\n",
    "    \n",
    "        data_grouped = self.data.groupby('entreprise')['text'].apply(tokenize)\n",
    "        \n",
    "        # Count the frequency of each word for each entreprise\n",
    "        word_counts = data_grouped.apply(lambda x: Counter(word for doc in x for word in doc))\n",
    "        \n",
    "        # Concatenate the word counts into a single DataFrame\n",
    "        dfs = []\n",
    "        for entreprise in word_counts.index:\n",
    "            counts = pd.DataFrame.from_dict(word_counts[entreprise], orient='index', columns=['count'])\n",
    "            counts['Entreprise'] = entreprise\n",
    "            dfs.append(counts)\n",
    "        df = pd.concat(dfs)\n",
    "        \n",
    "        # Sort the DataFrame by count in descending order and get the top n rows for each entreprise\n",
    "        df = df.sort_values(['Entreprise', 'count'], ascending=[True, False]).groupby('Entreprise').head(top_n)\n",
    "        \n",
    "        # Convert the index to a column and reset the index\n",
    "        df.reset_index(inplace=True)\n",
    "        df.rename(columns={'index': 'mots'}, inplace=True)\n",
    "        self.frequence_mots = df\n",
    "        return self.frequence_mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mots</th>\n",
       "      <th>count</th>\n",
       "      <th>Entreprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hydrogene</td>\n",
       "      <td>50</td>\n",
       "      <td>Air liquide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>france</td>\n",
       "      <td>27</td>\n",
       "      <td>Air liquide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>actions</td>\n",
       "      <td>22</td>\n",
       "      <td>Air liquide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stations</td>\n",
       "      <td>22</td>\n",
       "      <td>Air liquide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gaz</td>\n",
       "      <td>21</td>\n",
       "      <td>Air liquide</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>edf</td>\n",
       "      <td>168</td>\n",
       "      <td>TotalEnergies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>francais</td>\n",
       "      <td>150</td>\n",
       "      <td>TotalEnergies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>contrat</td>\n",
       "      <td>149</td>\n",
       "      <td>TotalEnergies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>projet</td>\n",
       "      <td>140</td>\n",
       "      <td>TotalEnergies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>gaz</td>\n",
       "      <td>130</td>\n",
       "      <td>TotalEnergies</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         mots  count     Entreprise\n",
       "0   hydrogene     50    Air liquide\n",
       "1      france     27    Air liquide\n",
       "2     actions     22    Air liquide\n",
       "3    stations     22    Air liquide\n",
       "4         gaz     21    Air liquide\n",
       "..        ...    ...            ...\n",
       "95        edf    168  TotalEnergies\n",
       "96   francais    150  TotalEnergies\n",
       "97    contrat    149  TotalEnergies\n",
       "98     projet    140  TotalEnergies\n",
       "99        gaz    130  TotalEnergies\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stats = Stats_desc(data_fin)\n",
    "data_stats.stats()\n",
    "data_stats.frequence(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0efa8c74aad001ef25a6d80deef3616d04c277939308e305a946d7ec788eb1bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
