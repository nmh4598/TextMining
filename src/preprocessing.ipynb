{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%reset\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from unidecode import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pathlib import Path\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load(\"fr_core_news_sm\", disable=[\"parser\", \"ner\"])\n",
    "pd.options.mode.chained_assignment = None "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class pour nettoyer les données et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.n_tweet = 0  # Nombre de tweets  \n",
    "        self.n_tweet_doublons = 0 # Nombre de tweets doublons \n",
    "        self.n_de_mots = 0 # Nombre de mots dans tous les tweets\n",
    "        self.n_urls = 0  # Nombre de liens supprimés      \n",
    "        self.n_mentions = 0 # Nombre de mentions supprimés \n",
    "        self.n_hashtags = 0 # Nombre de hashtags supprimés \n",
    "        self.n_stopwords = 0 # Nombre de stopwords supprimés\n",
    "        self.entreprises = [\"LVMH\", \"L'Oréal\", \"Hermès\", \"TotalEnergies\", \n",
    "                            \"Sanofi\", \"Airbus\", \"Schneider Electric\", \"Capgemini\", \n",
    "                            \"Air liquide\", \"BNP Paribas\"]\n",
    " \n",
    "    def conservation_top_10(self):\n",
    "        \"\"\"\n",
    "            Conservation des 10 plus grosses entreprises du CAC40\n",
    "        \"\"\"\n",
    "        self.data = self.data[self.data.entreprise.isin(self.entreprises)]\n",
    "        self.n_tweet = len(self.data)\n",
    "\n",
    "    def compter_mots(self):\n",
    "        def compter_mots_chaque_ligne(text):\n",
    "            words = re.split(r\"[ ’'\\-]\", text)\n",
    "            return len(words)\n",
    "        \n",
    "        self.n_de_mots = self.data['text'].apply(compter_mots_chaque_ligne).sum()\n",
    "\n",
    "    def supprime_urls_hashtags_mentions_ponctuations_nombres(self):\n",
    "        \"\"\"\n",
    "        Supprime les liens https jusqu'à l'espace suivant\n",
    "        Supprime toutes les mentions (@) ainsi que la suite jusqu'à l'espace suivant\n",
    "        Supprime tous les Hashtags (#) ainsi que la suite jusqu'à l'espace suivant \n",
    "        Supprime tous les nombres\n",
    "        Supprime tous les pontuations\n",
    "        Supprime tous les accents\n",
    "        \"\"\"\n",
    "        url_regex = r'(http[s]?:\\/\\/\\S+)'\n",
    "        pattern = r'(@\\w+|#\\w+|http[s]?:\\/\\/\\S+)'\n",
    "        num_regex = r'\\d+'\n",
    "        \n",
    "        def clean(text):\n",
    "            items = re.findall(pattern, text)\n",
    "            mentions = [item for item in items if item.startswith('@')]\n",
    "            hashtags = [item for item in items if item.startswith('#')]\n",
    "            urls = [item for item in items if re.match(url_regex, item)]         \n",
    "            n_mentions = len(mentions)\n",
    "            n_hashtags = len(hashtags)\n",
    "            n_urls = len(urls)    \n",
    "            text = re.sub(pattern, '', text)\n",
    "            text = re.sub(num_regex, '', text)\n",
    "            punctuations = re.findall(r'[^\\w\\s]', text)  \n",
    "            n_pontuations = len(punctuations)\n",
    "            # Ice, faut remplace par l'espace \n",
    "            text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "                \n",
    "            return text, mentions, hashtags, urls,punctuations, n_mentions, n_hashtags, n_urls, n_pontuations\n",
    "\n",
    "        self.data['text'], self.data['mentions'], self.data['hashtags'], self.data['urls'], self.data['punctuations'], self.n_mentions, self.n_hashtags, self.n_urls,self.n_pontuations = zip(*self.data['text'].apply(clean))\n",
    "        self.data['text'] = self.data['text'].str.lower()\n",
    "        self.n_mentions = sum(self.n_mentions)\n",
    "        self.n_hashtags = sum(self.n_hashtags)\n",
    "        self.n_urls = sum(self.n_urls)\n",
    "        self.n_punctuations = sum(self.n_pontuations)\n",
    "        def remove_accents(text):\n",
    "            return unidecode(text)\n",
    "        self.data['text'] = self.data['text'].apply(remove_accents)\n",
    "\n",
    "    def supprime_les_stopwords(self):\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('french'))\n",
    "        liste_mot_non_voulu = [\"lvmh\", \"oreal\",\"oréal\",\"L’Oréal\", \"sanofi\", \"airbus\", \"totalenergie\", \"totalenergies\", \"total\", \"energie\", \"energies\", \"air\", \"liquide\", \"bnp\", \"paribas\", \"pariba\", \"airliquide\", \"ça\", \"ca\", \"faire\", \"moi\", \"deja\"]\n",
    "        list_sw = stop_words_txt[\"vides\"].to_list()\n",
    "        stop_words.update(liste_mot_non_voulu)\n",
    "        stop_words.update(list_sw)\n",
    "        \n",
    "        def supprime_stopwords(text):\n",
    "            words = re.split(r\"[ ’'\\-]\", text)\n",
    "            filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "            filtered_stop_words = [word for word in words if word.lower() in stop_words]\n",
    "            return ' '.join(filtered_words),filtered_stop_words, len(words) - len(filtered_words)\n",
    "\n",
    "        self.data['text'],self.data[\"stopword\"], n_stopwords = zip(*self.data['text'].apply(supprime_stopwords))\n",
    "        self.n_stopwords = sum(n_stopwords)\n",
    "    \n",
    "    def transform_normal(self):\n",
    "        \"\"\"       \n",
    "            Supprime tous les accents     \n",
    "            Supprime les tweets n'ayant plus de texte. \n",
    "        \"\"\"\n",
    "        self.data['text'] = self.data['text'].str.strip()\n",
    "        self.data = self.data[self.data['text'] != '']\n",
    "        self.data['date'] = pd.to_datetime(self.data['date']).dt.date\n",
    "        \n",
    "    def supprime_doublons(self):\n",
    "        \"\"\"\n",
    "            Supprime tous les doublons avec le même texte\n",
    "        \"\"\"\n",
    "        self.data = self.data.drop_duplicates([\"text\"])\n",
    "        self.n_tweet_sans_doublons = len(self.data)\n",
    "        self.n_tweet_doublons = self.n_tweet - self.n_tweet_sans_doublons\n",
    "    \n",
    "    def stats_desc(self):\n",
    "        \"\"\"\n",
    "            Retourne un dataframe contenant les résultats\n",
    "        \"\"\"\n",
    "        results_dict = {\n",
    "            \"Nombre de tweets\": [self.n_tweet],           \n",
    "            \"Nombre de liens\": [self.n_urls],\n",
    "            \"Nombre de mots\":[self.n_de_mots],\n",
    "            \"Nombre de stop word\":[self.n_stopwords],\n",
    "            \"Nombre de mentions\": [self.n_mentions],\n",
    "            \"Nombre de hashtags\": [self.n_hashtags],\n",
    "            \"Nombre de tweets doublons\": [self.n_tweet_doublons]\n",
    "\n",
    "            }\n",
    "        self.results_dict = pd.DataFrame(results_dict)\n",
    "        return self.results_dict\n",
    "    \n",
    "    def lemmatize(self, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "        self.data['text'] = self.data['text'].apply(lambda x: \" \".join([token.lemma_ for token in nlp(x) if token.pos_ in allowed_postags]))\n",
    "        \n",
    "    def sentiment_analysis(self):\n",
    "        def getSubjectivity(text):\n",
    "            \"\"\" \n",
    "                La propriété sentiment d'un objet TextBlob renvoie un tuple nommé de la \n",
    "                forme (subjectivité), où la subjectivité est un flottant compris entre 0,0 \n",
    "                et 1,0, indiquant la subjectivité du texte. Une subjectivité de 0,0 signifie \n",
    "                que le texte est très objectif et factuel, tandis qu'une subjectivité de \n",
    "                1,0 signifie que le texte est très subjectif et opiniâtre.\n",
    "            \"\"\"\n",
    "            return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "\n",
    "        def getPolarity(text):\n",
    "            \"\"\"\n",
    "                La propriété sentiment d'un objet TextBlob renvoie un tuple nommé de la forme \n",
    "                (polarity ), où polarity est un flottant compris entre -1.0 et 1.0, indiquant\n",
    "                la polarité de sentiment du texte. Une polarité de -1,0 est très négative,\n",
    "                0 est neutre et 1,0 est très positive.\n",
    "            \"\"\"\n",
    "            return TextBlob(text).sentiment.polarity\n",
    "\n",
    "        self.data[\"Subjectivité\"] = self.data[\"text\"].apply(getSubjectivity)\n",
    "        self.data[\"Polarité\"] = self.data[\"text\"].apply(getPolarity)\n",
    "\n",
    "\n",
    "        def getAnalysis_polarity(score):\n",
    "            if score < 0:\n",
    "                return \"Negative\"\n",
    "            elif score == 0:\n",
    "                return \"Neutral\"\n",
    "            else:\n",
    "                return \"Positive\"\n",
    "        def getAnalysis_subjectivity(score):\n",
    "            if score < 0.5:\n",
    "                return \"Très objectif et factuel\"\n",
    "            else:\n",
    "                return \"Très subjectif et opiniâtre\"\n",
    "        self.data[\"Subjectivité\"] = self.data[\"Subjectivité\"].apply(getAnalysis_subjectivity)\n",
    "        self.data[\"Polarité\"] = self.data[\"Polarité\"].apply(getAnalysis_polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):  \n",
    "    data_processor = DataProcessor(data)\n",
    "    data_processor.conservation_top_10()\n",
    "    data_processor.compter_mots()\n",
    "    data_processor.supprime_urls_hashtags_mentions_ponctuations_nombres()\n",
    "    data_processor.lemmatize() # Dans le cas que on veut lemmatizer les données \n",
    "    data_processor.supprime_les_stopwords()\n",
    "    data_processor.transform_normal()\n",
    "    data_processor.supprime_doublons()\n",
    "    data_processor.sentiment_analysis()\n",
    "    data_fin = data_processor.data\n",
    "    data_processor.stats_desc()\n",
    "    results = data_processor.results_dict\n",
    "    return data_fin, results "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lire les fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "path_input = Path.cwd().parent.joinpath('data')\n",
    "\n",
    "path_data_input = path_input.joinpath('data_init.parquet')\n",
    "path_stopwords = path_input.joinpath('stop_words_french.txt')\n",
    "\n",
    "stop_words_txt = pd.read_table(path_stopwords, header = 0)\n",
    "data_input = pd.read_parquet(path_data_input) \n",
    "\n",
    "data_fin, results = transform(data_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = Path.cwd().parent.joinpath('outputs')\n",
    "\n",
    "path_data_output = path_output.joinpath('data_output.parquet')\n",
    "path_results = path_output.joinpath('results.parquet')\n",
    "\n",
    "data_fin.to_parquet(path_data_output)\n",
    "results.to_parquet(path_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0efa8c74aad001ef25a6d80deef3616d04c277939308e305a946d7ec788eb1bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
